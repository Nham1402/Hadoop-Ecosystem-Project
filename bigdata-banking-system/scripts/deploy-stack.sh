#!/bin/bash

# Big Data Banking System - Stack Deployment Script
# Cháº¡y script nÃ y trÃªn Master Node Ä‘á»ƒ triá»ƒn khai toÃ n bá»™ há»‡ thá»‘ng

set -e

echo "=========================================="
echo "TRIá»‚N KHAI Há»† THá»NG BIG DATA BANKING"
echo "=========================================="

# Kiá»ƒm tra prerequisite
echo "ğŸ” Kiá»ƒm tra Ä‘iá»u kiá»‡n cáº§n thiáº¿t..."

# Kiá»ƒm tra Docker Swarm
if ! docker info --format '{{.Swarm.LocalNodeState}}' | grep -q "active"; then
    echo "âŒ Docker Swarm chÆ°a Ä‘Æ°á»£c khá»Ÿi táº¡o!"
    echo "Cháº¡y ./scripts/init-swarm.sh trÆ°á»›c"
    exit 1
fi

# Kiá»ƒm tra sá»‘ lÆ°á»£ng nodes
EXPECTED_NODES=3
CURRENT_NODES=$(docker node ls --format "table {{.ID}}" | wc -l)
CURRENT_NODES=$((CURRENT_NODES - 1))

if [ $CURRENT_NODES -lt $EXPECTED_NODES ]; then
    echo "âŒ KhÃ´ng Ä‘á»§ nodes! Cáº§n $EXPECTED_NODES nodes"
    exit 1
fi

# Kiá»ƒm tra file cáº¥u hÃ¬nh
if [ ! -f ".env" ]; then
    echo "âŒ File .env khÃ´ng tá»“n táº¡i!"
    echo "Táº¡o file .env tá»« template"
    exit 1
fi

if [ ! -f "docker-compose.yml" ]; then
    echo "âŒ File docker-compose.yml khÃ´ng tá»“n táº¡i!"
    exit 1
fi

echo "âœ… Táº¥t cáº£ Ä‘iá»u kiá»‡n Ä‘Ã£ sáºµn sÃ ng"

# Load environment variables
source .env

echo ""
echo "ğŸ“‹ ThÃ´ng tin cáº¥u hÃ¬nh:"
echo "   Master IP: $MASTER_IP"
echo "   Worker 1 IP: $WORKER1_IP"  
echo "   Worker 2 IP: $WORKER2_IP"
echo "   MySQL Database: $MYSQL_DATABASE"
echo ""

# Táº¡o cÃ¡c file cáº¥u hÃ¬nh cáº§n thiáº¿t
echo "ğŸ“ Táº¡o cÃ¡c file cáº¥u hÃ¬nh..."

# MySQL init script
cat > configs/mysql/init.sql << 'EOF'
-- Banking Metadata Database Initialization

CREATE DATABASE IF NOT EXISTS banking_metadata;
USE banking_metadata;

-- Airflow tables will be created automatically

-- Hive Metastore tables
CREATE TABLE IF NOT EXISTS SEQUENCE_TABLE (
    SEQUENCE_NAME VARCHAR(255) NOT NULL,
    NEXT_VAL BIGINT NOT NULL,
    PRIMARY KEY (SEQUENCE_NAME)
);

-- Insert initial sequence values
INSERT IGNORE INTO SEQUENCE_TABLE (SEQUENCE_NAME, NEXT_VAL) VALUES 
('org.apache.hadoop.hive.metastore.model.MDatabase', 1),
('org.apache.hadoop.hive.metastore.model.MTable', 1),
('org.apache.hadoop.hive.metastore.model.MPartition', 1);

-- Banking specific tables
CREATE TABLE IF NOT EXISTS banking_customers (
    customer_id VARCHAR(20) PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    email VARCHAR(100),
    phone VARCHAR(20),
    address TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS banking_accounts (
    account_id VARCHAR(20) PRIMARY KEY,
    customer_id VARCHAR(20),
    account_type VARCHAR(20),
    balance DECIMAL(15,2),
    status VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (customer_id) REFERENCES banking_customers(customer_id)
);

CREATE TABLE IF NOT EXISTS banking_transactions (
    transaction_id VARCHAR(30) PRIMARY KEY,
    from_account VARCHAR(20),
    to_account VARCHAR(20),
    amount DECIMAL(15,2),
    transaction_type VARCHAR(20),
    status VARCHAR(20),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Insert sample data
INSERT IGNORE INTO banking_customers VALUES 
('CUST001', 'Nguyen', 'Van A', 'nva@email.com', '0901234567', 'Ha Noi', NOW()),
('CUST002', 'Tran', 'Thi B', 'ttb@email.com', '0902345678', 'Ho Chi Minh', NOW()),
('CUST003', 'Le', 'Van C', 'lvc@email.com', '0903456789', 'Da Nang', NOW());

INSERT IGNORE INTO banking_accounts VALUES 
('ACC001', 'CUST001', 'CHECKING', 50000000.00, 'ACTIVE', NOW()),
('ACC002', 'CUST002', 'SAVINGS', 75000000.00, 'ACTIVE', NOW()),
('ACC003', 'CUST003', 'CHECKING', 30000000.00, 'ACTIVE', NOW());

INSERT IGNORE INTO banking_transactions VALUES 
('TXN001', 'ACC001', 'ACC002', 1000000.00, 'TRANSFER', 'COMPLETED', NOW()),
('TXN002', 'ACC002', 'ACC003', 500000.00, 'TRANSFER', 'COMPLETED', NOW()),
('TXN003', 'ACC001', 'ACC003', 2000000.00, 'TRANSFER', 'PENDING', NOW());

EOF

# Hadoop core-site.xml
cat > configs/hadoop/core-site.xml << 'EOF'
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://namenode:8020</value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>root</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
</configuration>
EOF

# Hadoop hdfs-site.xml
cat > configs/hadoop/hdfs-site.xml << 'EOF'
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/hadoop/dfs/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/hadoop/dfs/data</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.webhdfs.enabled</name>
        <value>true</value>
    </property>
</configuration>
EOF

# Spark configuration
cat > configs/spark/spark-defaults.conf << 'EOF'
spark.master                     spark://spark-master:7077
spark.eventLog.enabled           true
spark.eventLog.dir               hdfs://namenode:8020/spark-logs
spark.sql.warehouse.dir          hdfs://namenode:8020/spark-warehouse
spark.serializer                 org.apache.spark.serializer.KryoSerializer
spark.sql.adaptive.enabled       true
spark.sql.adaptive.coalescePartitions.enabled true
spark.hadoop.fs.defaultFS        hdfs://namenode:8020
spark.sql.catalogImplementation  hive
spark.sql.hive.metastore.version 3.1.3
spark.sql.hive.metastore.jars    builtin
EOF

# Prometheus configuration
cat > monitoring/prometheus.yml << 'EOF'
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'hadoop-namenode'
    static_configs:
      - targets: ['namenode:9870']

  - job_name: 'hadoop-resourcemanager'
    static_configs:
      - targets: ['resourcemanager:8088']

  - job_name: 'spark-master'
    static_configs:
      - targets: ['spark-master:8080']

  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka-1:29092', 'kafka-2:29093', 'kafka-3:29094']
EOF

echo "âœ… ÄÃ£ táº¡o cÃ¡c file cáº¥u hÃ¬nh"

# Pull cÃ¡c images trÆ°á»›c khi deploy
echo ""
echo "ğŸ³ Pull Docker images..."
docker pull mysql:8.0
docker pull confluentinc/cp-zookeeper:7.4.0
docker pull confluentinc/cp-kafka:7.4.0
docker pull apache/hadoop:3.3.6
docker pull bitnami/spark:3.5
docker pull apache/airflow:2.7.2

echo "âœ… ÄÃ£ pull cÃ¡c Docker images cáº§n thiáº¿t"

# Deploy stack
echo ""
echo "ğŸš€ Triá»ƒn khai Big Data Banking Stack..."
echo "TÃªn stack: banking-stack"

docker stack deploy -c docker-compose.yml banking-stack

echo ""
echo "â³ Äá»£i cÃ¡c services khá»Ÿi Ä‘á»™ng..."

# Äá»£i cÃ¡c services cÆ¡ báº£n
sleep 30

echo ""
echo "ğŸ“Š Kiá»ƒm tra tráº¡ng thÃ¡i services:"
docker service ls

echo ""
echo "ğŸ” Kiá»ƒm tra tráº¡ng thÃ¡i containers:"
docker stack ps banking-stack

# Äá»£i MySQL vÃ  Zookeeper khá»Ÿi Ä‘á»™ng
echo ""
echo "â³ Äá»£i MySQL vÃ  Zookeeper khá»Ÿi Ä‘á»™ng hoÃ n táº¥t..."
sleep 60

# Kiá»ƒm tra health cá»§a cÃ¡c services chÃ­nh
echo ""
echo "ğŸ¥ Kiá»ƒm tra health cÃ¡c services chÃ­nh..."

check_service() {
    local service_name=$1
    local port=$2
    local host=${3:-localhost}
    
    echo -n "Checking $service_name..."
    if curl -s -f http://$host:$port > /dev/null 2>&1; then
        echo " âœ…"
    else
        echo " âŒ"
    fi
}

echo ""
echo "Äá»£i thÃªm 60s Ä‘á»ƒ services hoÃ n táº¥t khá»Ÿi Ä‘á»™ng..."
sleep 60

# Táº¡o Kafka topics cho banking
echo ""
echo "ğŸ“ Táº¡o Kafka topics cho banking system..."

# Äá»£i Kafka cluster sáºµn sÃ ng
sleep 30

# Táº¡o topics
docker exec -i $(docker ps -q -f name=banking-stack_kafka-1) kafka-topics --create --topic banking-transactions --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3 --if-not-exists
docker exec -i $(docker ps -q -f name=banking-stack_kafka-1) kafka-topics --create --topic fraud-alerts --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3 --if-not-exists  
docker exec -i $(docker ps -q -f name=banking-stack_kafka-1) kafka-topics --create --topic risk-events --bootstrap-server localhost:29092 --partitions 3 --replication-factor 3 --if-not-exists

echo "âœ… ÄÃ£ táº¡o Kafka topics"

# Khá»Ÿi táº¡o HDFS directories
echo ""
echo "ğŸ“ Khá»Ÿi táº¡o HDFS directories..."
sleep 10

docker exec -i $(docker ps -q -f name=banking-stack_namenode) hdfs dfsadmin -safemode leave || true
docker exec -i $(docker ps -q -f name=banking-stack_namenode) hdfs dfs -mkdir -p /user/hive/warehouse || true
docker exec -i $(docker ps -q -f name=banking-stack_namenode) hdfs dfs -mkdir -p /spark-logs || true
docker exec -i $(docker ps -q -f name=banking-stack_namenode) hdfs dfs -mkdir -p /banking-data || true
docker exec -i $(docker ps -q -f name=banking-stack_namenode) hdfs dfs -chmod 777 /user/hive/warehouse || true
docker exec -i $(docker ps -q -f name=banking-stack_namenode) hdfs dfs -chmod 777 /spark-logs || true

echo "âœ… ÄÃ£ khá»Ÿi táº¡o HDFS directories"

echo ""
echo "=========================================="
echo "TRIá»‚N KHAI HOÃ€N Táº¤T!"
echo "=========================================="
echo ""
echo "ğŸ‰ Há»‡ thá»‘ng Big Data Banking Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai thÃ nh cÃ´ng!"
echo ""
echo "ğŸŒ CÃC URL TRUY Cáº¬P (tá»« bÃªn ngoÃ i):"
echo ""
echo "ğŸ“Š HADOOP ECOSYSTEM:"
echo "   â€¢ NameNode WebUI:      http://$MASTER_IP:9870"
echo "   â€¢ YARN ResourceMgr:    http://$MASTER_IP:8088"
echo ""
echo "âš¡ SPARK:"
echo "   â€¢ Master WebUI:        http://$MASTER_IP:8080"
echo ""
echo "ğŸ›ï¸  HBASE:"
echo "   â€¢ Master WebUI:        http://$MASTER_IP:16010"
echo ""
echo "ğŸ“¨ KAFKA:"
echo "   â€¢ Kafka UI:            http://$MASTER_IP:8090"
echo "   â€¢ Schema Registry:     http://$MASTER_IP:8085"
echo ""
echo "ğŸ”„ AIRFLOW:"
echo "   â€¢ Web Interface:       http://$MASTER_IP:8081"
echo "   â€¢ Username/Password:   $AIRFLOW_USER/$AIRFLOW_PASSWORD"
echo ""
echo "ğŸ“Š MONITORING:"
echo "   â€¢ Grafana:             http://$MASTER_IP:3000"
echo "   â€¢ Username/Password:   admin/$GRAFANA_PASSWORD"
echo "   â€¢ Prometheus:          http://$MASTER_IP:9090"
echo ""
echo "ğŸ”¬ DEVELOPMENT:"
echo "   â€¢ Jupyter Notebook:    http://$MASTER_IP:8888"
echo ""
echo "ğŸ—„ï¸  DATABASE:"
echo "   â€¢ MySQL:               $MASTER_IP:3306"
echo "   â€¢ Database:            $MYSQL_DATABASE"
echo "   â€¢ Username/Password:   $MYSQL_USER/$MYSQL_PASSWORD"
echo ""
echo "ğŸ“‹ TRáº NG THÃI SERVICES:"
docker service ls
echo ""
echo "ğŸ’¡ LÆ¯U Ã:"
echo "   â€¢ Má»™t sá»‘ services cÃ³ thá»ƒ cáº§n thÃªm thá»i gian Ä‘á»ƒ khá»Ÿi Ä‘á»™ng hoÃ n toÃ n"
echo "   â€¢ Kiá»ƒm tra logs náº¿u cÃ³ service nÃ o lá»—i: docker service logs <service_name>"
echo "   â€¢ Jupyter token cÃ³ thá»ƒ láº¥y tá»«: docker service logs banking-stack_jupyter"
echo ""
echo "ğŸš€ Há»‡ thá»‘ng Ä‘Ã£ sáºµn sÃ ng xá»­ lÃ½ dá»¯ liá»‡u banking!"