# HÆ¯á»šNG DáºªN CÃ€I Äáº¶T THá»°C Táº¾ CHO Há»† THá»NG HADOOP E-COMMERCE

## ğŸš¨ QUAN TRá»ŒNG: MÃ´i trÆ°á»ng cÃ i Ä‘áº·t

Äá»ƒ cÃ i Ä‘áº·t thÃ nh cÃ´ng há»‡ thá»‘ng Hadoop E-commerce Ecosystem, báº¡n cáº§n:

### **YÃªu cáº§u há»‡ thá»‘ng:**
- **3 mÃ¡y Ubuntu 20.04/22.04 LTS** (váº­t lÃ½ hoáº·c VM)
- **1 Master Node**: 6GB RAM, 3 CPU cores, 100GB disk
- **2 Worker Nodes**: 4GB RAM, 2 CPU cores, 200GB disk má»—i mÃ¡y
- **Káº¿t ná»‘i máº¡ng** giá»¯a cÃ¡c mÃ¡y
- **Quyá»n sudo** trÃªn táº¥t cáº£ cÃ¡c mÃ¡y

---

## ğŸ“‹ BÆ¯á»šC 1: CHUáº¨N Bá»Š Táº¤T Cáº¢ CÃC NODES

### TrÃªn táº¥t cáº£ 3 mÃ¡y, cháº¡y:

```bash
# 1. Cáº­p nháº­t há»‡ thá»‘ng
sudo apt update && sudo apt upgrade -y

# 2. Clone repository
git clone https://github.com/your-repo/hadoop-ecommerce-ecosystem.git
cd hadoop-ecommerce-ecosystem

# 3. CÃ i Ä‘áº·t cÃ¡c á»©ng dá»¥ng cáº§n thiáº¿t
chmod +x scripts/setup/install-prerequisites.sh
./scripts/setup/install-prerequisites.sh
```

### Thiáº¿t láº­p hostname vÃ  hosts file:

```bash
# TrÃªn Master Node
sudo hostnamectl set-hostname master-node

# TrÃªn Worker Node 1
sudo hostnamectl set-hostname worker-node-1

# TrÃªn Worker Node 2
sudo hostnamectl set-hostname worker-node-2

# TrÃªn táº¥t cáº£ nodes, thÃªm vÃ o /etc/hosts:
sudo nano /etc/hosts
# ThÃªm:
192.168.1.10  master-node
192.168.1.11  worker-node-1
192.168.1.12  worker-node-2
```

---

## ğŸ¯ BÆ¯á»šC 2: CÃ€I Äáº¶T MASTER NODE

### TrÃªn mÃ¡y Master Node (6GB RAM, 3 CPU):

```bash
# 1. Cháº¡y script cÃ i Ä‘áº·t master
chmod +x scripts/setup/install-master-node.sh
./scripts/setup/install-master-node.sh

# 2. Láº¥y lá»‡nh join cho worker nodes
sudo kubeadm token create --print-join-command

# LÆ°u lá»‡nh nÃ y Ä‘á»ƒ sá»­ dá»¥ng trÃªn worker nodes
```

### Kiá»ƒm tra Master Node:

```bash
kubectl get nodes
kubectl get pods -n kube-system
```

---

## ğŸ‘¥ BÆ¯á»šC 3: CÃ€I Äáº¶T WORKER NODES

### TrÃªn má»—i Worker Node (4GB RAM, 2 CPU):

```bash
# 1. Clone repository (náº¿u chÆ°a cÃ³)
git clone https://github.com/your-repo/hadoop-ecommerce-ecosystem.git
cd hadoop-ecommerce-ecosystem

# 2. Cháº¡y script cÃ i Ä‘áº·t worker vá»›i lá»‡nh join tá»« master
chmod +x scripts/setup/install-worker-node.sh
./scripts/setup/install-worker-node.sh "sudo kubeadm join 192.168.1.10:6443 --token <token> --discovery-token-ca-cert-hash <hash>"
```

### Tá»« Master Node, cáº¥u hÃ¬nh worker nodes:

```bash
# Gáº¯n nhÃ£n cho worker nodes
kubectl label node worker-node-1 node-role=worker
kubectl label node worker-node-2 node-role=worker

# Kiá»ƒm tra táº¥t cáº£ nodes
kubectl get nodes -o wide
```

---

## ğŸš€ BÆ¯á»šC 4: TRIá»‚N KHAI HADOOP ECOSYSTEM

### Tá»« Master Node:

```bash
# 1. Táº¡o namespace vÃ  cáº¥u hÃ¬nh
kubectl create namespace hadoop-ecosystem

# 2. Build Docker images (náº¿u cáº§n)
make build

# 3. Triá»ƒn khai toÃ n bá»™ há»‡ thá»‘ng
make deploy

# Hoáº·c triá»ƒn khai tá»«ng bÆ°á»›c:
./scripts/deploy/deploy-all.sh
```

### Kiá»ƒm tra triá»ƒn khai:

```bash
# Xem táº¥t cáº£ pods
kubectl get pods -n hadoop-ecosystem

# Xem services
kubectl get services -n hadoop-ecosystem

# Cháº¡y health check
./scripts/operations/health-check.sh
```

---

## ğŸŒ BÆ¯á»šC 5: TRUY Cáº¬P Há»† THá»NG

### Cáº¥u hÃ¬nh truy cáº­p tá»« bÃªn ngoÃ i:

```bash
# TrÃªn Master Node, cáº¥u hÃ¬nh port forwarding:

# Hadoop NameNode
kubectl port-forward -n hadoop-ecosystem svc/hadoop-namenode 9870:9870 &

# Spark Master
kubectl port-forward -n hadoop-ecosystem svc/spark-master 8080:8080 &

# YARN ResourceManager
kubectl port-forward -n hadoop-ecosystem svc/yarn-resourcemanager 8088:8088 &

# Grafana Monitoring
kubectl port-forward -n hadoop-ecosystem svc/grafana 3000:3000 &
```

### Hoáº·c sá»­ dá»¥ng NodePort (truy cáº­p qua IP cá»§a nodes):

- **Hadoop NameNode**: `http://<master-ip>:30870`
- **YARN ResourceManager**: `http://<master-ip>:30088`
- **Spark Master**: `http://<master-ip>:30080`
- **Grafana**: `http://<master-ip>:30300`

---

## ğŸ“Š BÆ¯á»šC 6: LOAD Dá»® LIá»†U VÃ€ CHáº Y JOBS

### Load dá»¯ liá»‡u máº«u:

```bash
# Tá»« Master Node
make load-data

# Hoáº·c thá»§ cÃ´ng:
./scripts/data/sample-data-loader.sh
```

### Cháº¡y Spark job máº«u:

```bash
# Submit daily sales report job
kubectl exec -n hadoop-ecosystem deployment/spark-master -- \
  spark-submit \
  --master spark://spark-master:7077 \
  --deploy-mode client \
  /opt/spark/jobs/batch-processing/daily_sales_report.py
```

### KÃ­ch hoáº¡t Airflow DAG:

1. Truy cáº­p Airflow UI: `http://<master-ip>:8080`
2. Login vá»›i `admin/admin`
3. Enable DAG `daily_etl_pipeline`
4. Trigger manual run

---

## ğŸ”§ SCRIPTS Tá»° Äá»˜NG ÄÃƒ Táº O

### Scripts cÃ i Ä‘áº·t:

```bash
scripts/setup/install-prerequisites.sh     # CÃ i Docker, K8s, Helm, Java
scripts/setup/install-master-node.sh       # CÃ i Ä‘áº·t Master Node
scripts/setup/install-worker-node.sh       # CÃ i Ä‘áº·t Worker Node
scripts/setup/setup-cluster.sh             # Cáº¥u hÃ¬nh cluster
```

### Scripts triá»ƒn khai:

```bash
scripts/deploy/deploy-all.sh               # Deploy toÃ n bá»™ há»‡ thá»‘ng
scripts/deploy/deploy-hadoop.sh            # Deploy chá»‰ Hadoop
scripts/deploy/deploy-spark.sh             # Deploy chá»‰ Spark
scripts/deploy/deploy-monitoring.sh        # Deploy monitoring
```

### Scripts quáº£n lÃ½:

```bash
scripts/operations/health-check.sh         # Kiá»ƒm tra sá»©c khá»e há»‡ thá»‘ng
scripts/operations/scale-workers.sh        # Scale workers
scripts/operations/backup-data.sh          # Backup dá»¯ liá»‡u
scripts/operations/cleanup.sh              # Dá»n dáº¹p
```

---

## ğŸ“ˆ GIÃM SÃT VÃ€ QUáº¢N LÃ

### Monitoring vá»›i Grafana:

```bash
# Truy cáº­p Grafana
kubectl port-forward -n hadoop-ecosystem svc/grafana 3000:3000

# Má»Ÿ browser: http://localhost:3000
# Login: admin/admin
```

### Kiá»ƒm tra logs:

```bash
# Logs cá»§a NameNode
kubectl logs -f deployment/hadoop-namenode -n hadoop-ecosystem

# Logs cá»§a Spark Master
kubectl logs -f deployment/spark-master -n hadoop-ecosystem

# Logs cá»§a táº¥t cáº£ Hadoop components
kubectl logs -f -l app=hadoop -n hadoop-ecosystem
```

### Health check Ä‘á»‹nh ká»³:

```bash
# Cháº¡y health check
./scripts/operations/health-check.sh

# Táº¡o cron job Ä‘á»ƒ cháº¡y hÃ ng ngÃ y
echo "0 8 * * * /path/to/scripts/operations/health-check.sh" | crontab -
```

---

## ğŸ› ï¸ TROUBLESHOOTING

### Váº¥n Ä‘á» thÆ°á»ng gáº·p:

#### 1. Pods khÃ´ng khá»Ÿi Ä‘á»™ng:
```bash
kubectl describe pod <pod-name> -n hadoop-ecosystem
kubectl logs <pod-name> -n hadoop-ecosystem
```

#### 2. Nodes khÃ´ng join Ä‘Æ°á»£c:
```bash
# TrÃªn worker node:
sudo systemctl status kubelet
sudo journalctl -xeu kubelet

# Reset vÃ  thá»­ láº¡i:
sudo kubeadm reset
```

#### 3. Docker daemon khÃ´ng cháº¡y:
```bash
sudo systemctl status docker
sudo systemctl start docker
sudo systemctl enable docker
```

#### 4. Thiáº¿u tÃ i nguyÃªn:
```bash
# Kiá»ƒm tra resource usage
kubectl top nodes
kubectl top pods -n hadoop-ecosystem

# Scale down náº¿u cáº§n
kubectl scale deployment <deployment-name> --replicas=1 -n hadoop-ecosystem
```

---

## ğŸ“‹ CHECKLIST CÃ€I Äáº¶T

### âœ… Chuáº©n bá»‹:
- [ ] 3 mÃ¡y Ubuntu vá»›i cáº¥u hÃ¬nh Ä‘á»§
- [ ] Káº¿t ná»‘i máº¡ng giá»¯a cÃ¡c mÃ¡y
- [ ] Quyá»n sudo trÃªn táº¥t cáº£ mÃ¡y
- [ ] Repository Ä‘Ã£ Ä‘Æ°á»£c clone

### âœ… Master Node:
- [ ] Docker Ä‘Ã£ cÃ i Ä‘áº·t vÃ  cháº¡y
- [ ] Kubernetes tools Ä‘Ã£ cÃ i Ä‘áº·t
- [ ] Cluster Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o
- [ ] kubectl hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng
- [ ] CNI (Flannel) Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t

### âœ… Worker Nodes:
- [ ] Docker vÃ  K8s tools Ä‘Ã£ cÃ i Ä‘áº·t
- [ ] ÄÃ£ join cluster thÃ nh cÃ´ng
- [ ] Nodes cÃ³ label phÃ¹ há»£p
- [ ] kubelet Ä‘ang cháº¡y

### âœ… Hadoop Ecosystem:
- [ ] Namespace hadoop-ecosystem Ä‘Ã£ táº¡o
- [ ] ConfigMaps vÃ  Secrets Ä‘Ã£ apply
- [ ] Storage Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh
- [ ] Táº¥t cáº£ pods Ä‘ang cháº¡y
- [ ] Services cÃ³ endpoints

### âœ… Kiá»ƒm tra cuá»‘i:
- [ ] Web UIs cÃ³ thá»ƒ truy cáº­p
- [ ] Sample data Ä‘Ã£ Ä‘Æ°á»£c load
- [ ] Spark jobs cháº¡y thÃ nh cÃ´ng
- [ ] Monitoring hoáº¡t Ä‘á»™ng
- [ ] Health check pass

---

## ğŸ¯ Káº¾T QUáº¢ MONG Äá»¢I

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:

âœ… **Kubernetes cluster** vá»›i 1 master + 2 workers  
âœ… **Hadoop HDFS** vá»›i NameNode vÃ  2 DataNodes  
âœ… **Apache Spark** vá»›i Master vÃ  2 Workers  
âœ… **Apache Kafka** cho real-time streaming  
âœ… **Apache Hive** cho data warehousing  
âœ… **Apache HBase** cho NoSQL storage  
âœ… **Apache Airflow** cho workflow orchestration  
âœ… **Monitoring stack** vá»›i Prometheus + Grafana  
âœ… **Sample e-commerce data** vÃ  analytics jobs  

**Tá»•ng tÃ i nguyÃªn**: 14GB RAM, 7 CPU cores nhÆ° yÃªu cáº§u ban Ä‘áº§u!

---

## ğŸ“ Há»– TRá»¢

Náº¿u gáº·p váº¥n Ä‘á»:

1. **Kiá»ƒm tra logs**: `kubectl logs <pod-name> -n hadoop-ecosystem`
2. **Cháº¡y health check**: `./scripts/operations/health-check.sh`
3. **Xem events**: `kubectl get events -n hadoop-ecosystem`
4. **Kiá»ƒm tra tÃ i nguyÃªn**: `kubectl top nodes && kubectl top pods -n hadoop-ecosystem`

**LÆ°u Ã½**: Há»‡ thá»‘ng nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cháº¡y trÃªn mÃ´i trÆ°á»ng thá»±c vá»›i Docker vÃ  Kubernetes Ä‘áº§y Ä‘á»§. Trong mÃ´i trÆ°á»ng container/sandbox cÃ³ thá»ƒ gáº·p háº¡n cháº¿ vá» systemd vÃ  kernel modules.